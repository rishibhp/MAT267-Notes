% TODO: Add sidebar about rotation groups

\section{Inhomogeneous Linear Systems}
Suppose we have an equation of the form
\begin{equation}\label{eq:inhom-ode}
    X' = Ax + f(t)
\end{equation}
Then we know from the superposition principle (see \autoref{sec:superposition-principle}) that the general solution to this system is given by
$$ X(t) = y(t) + e^{tA}v $$
where $y(t)$ is a particular solution to the ODE and $v$ is some arbitrary vector that is determined by the initial conditions. Thus our goal is to find just \textit{one} solution to this ODE. This leads us to Duhamel's Principle.

\subsection{Duhamel's Principle}
We will guess that $y(t) = e^{tA} v(t)$ is a solution where $v(t)$ is a function to be determined (this is the technique of variation of constants). Assuming $y$ is a solution we can plug this in \autoref{eq:inhom-ode} to find the left and right hand sides are
\begin{align}
    y' &= Ae^{tA} v(t) + e^{tA} v'(t) \tag{LHS}\\
    Ay + f(t) &= Ae^{tA} v(t) + f(t) \tag{RHS}
\end{align}
Equating the two we find that
$$ v'(t) = e^{-tA} f(t) $$
Hence by the Fundamental Theorem of Calculus we find that 
$$ v(t) = \int_{0}^{t} e^{-sA} f(s) ds $$
(recall we only need a particular solution so we can ignore the constant of integration by setting it to 0). 
Therefore our particular solution $y$ is given by
$$ y(t) = e^{tA} v(t) = e^{tA} \int_{0}^t e^{-sA} f(s) ds = \int_{0}^t e^{(t - s)A} f(s) ds $$
\begin{remark}
Note that $t - s$ is always positive. Although not particularly relevant to this example, this is an important note in other contexts.
\end{remark}
\begin{remark}
What we mean by integrating a vector-valued function is to integrate each of the component functions and `stack' them together to get another vector.
\end{remark}

\section{Linearisation}
The idea with linearisation is to simply use the framework we've 
Suppose $f: \R^n \to \R^n$ is a smooth vector field and we have the differential equation
\begin{equation}\label{eq:linsation-eg}
    x' = f(x)
\end{equation}
Although it's hard to give an explicit solution to this, we can still try and determine it's qualitative behaviour, at least locally. Let $p \in \R^n$ be arbitrary. If $f(p)$ is non-zero, then for $q$ near $p$ we are going to have that $f(q)$ is close to $f(p)$. Therefore the flow is going to look like (almost) parallel lines in this neighbourhood. If $f(p)$ is 0, we need to do a bit more work. One thing we can do is consider the Taylor expansion of $f$. We know that
$$ f(x) = f(p) + f'(p)(x - p) + \underbrace{O(|x - p|)}_{\text{error}} $$
Since we assume $f(p) = 0$, the first term disappears and by substituting $y = x - p$, \autoref{eq:linsation-eg} becomes
$$ y' = Df(p)y + O(|y|) $$
Since $Df(p)$ is a linear map, this looks just like the linear equations we have studied thus far, except there is the added error term. The hope is that this error terms is going to be small so by studying the linear system
$$ y' = Df(p) y $$
we can get a pretty good idea of how the true system behaves. For example if there are sources or sinks in the linearised system, they will also appear in the true system. The phase portraits will also be similar (similarity will be defined more precisely later) if the origin is hyperbolic (recall this means that $\Re(\lambda) \neq 0$ for every eigenvalue $\lambda$ of $Df(p)$).

This determines a procedure that we can use to study such equations
\begin{minenumerate}
    \item Find the steady states/equlibria (the points where $x' = f(x)$ is 0). These are points where the solutions are constant and don't change over time.
    \item Linearise near these steady states
    \item Tie everything up into a big picture
\end{minenumerate}
The cryptic 'tie everything up into a big picture' can be best illustrated with an example.

\subsection{Example}
We will consider the case of swinging a pendulum, kind of. More precisely we will look at the case when a mass is attached to a (rigid) rod allowed to swing freely in a vertical circle (we use a rod instead of a string because we don't want to worry about cases where the string may fold onto itself or something). 

% TODO: Insert mass pendulumn diagram

The equation modelling this situation is given by
\begin{equation*}
    mx'' + rx' = -c \sin x
\end{equation*}
where $x$ is the angle made by the rod with the vertical. Here $m$ is the mass of the mass (names are difficult for physicists), $r$ is the constant of friction (therefore $r \geq 0$) and $c$ is some arbitrary constant (the exact details, such as length of the rod, strength of gravity, etc, are used to set $c$). To make the analysis a bit simpler we will assume $m$ and $c$ to be 1. Then in particular we have the equation
\begin{equation}
    x'' + rx' + \sin x = 0
\end{equation}
We can use our standard trick to convert this second order equation to a system of first order equation: let $v = x'$. Then we have
\begin{align*}
    \begin{cases}
    x' &= v\\
    v' &= -rv - \sin x
    \end{cases}
\end{align*}
We first find the the steady states or in other words where $v$ is 0 (this implies that $v'$ is 0 since $v' = \frac{df(x(t))}{dt} = f'(x) x'(t) = f'(x) v = 0$. These are the points where $\sin x = 0$ or in other words where $x = k\pi, k \in \Z$. $k$ being even corresponds with the mass hanging on the bottom and $k$ being odd is when the mass is at the top in a perfectly vertical position. Simple intuition tells us that the the former equlibria should be stable (at least if $r > 0$)and the latter should be unstable. Let us see if the equations agree with this. First we see that 
$$ \matrix{x\\v}' = f(x,v), f(x, v) = \matrix{v\\-rv - \sin x}$$
Then
\begin{align*}
    Df(k\pi, 0) &= \matrix{0 & 1\\-\cos x & -r}\Bigg|_{x = k\pi}\\
    &= 
    \begin{cases}
    \matrix{0 & 1\\1 & -r}, &k \text{ odd}\\
    \matrix{0 & 1\\-1 & -r}, &k \text{ even}
    \end{cases}
\end{align*}

Let us consider the case with $k$ odd first. In this case the determinant of the matrix is $-1$ so the eigenvalues are real and of opposite sign. We know this will result in a saddle and hence be unstable. In fact around these points we expect the phase portrait to (roughly) have a saddle as well. This lines up with our intuition above.

Now let us consider the case with $k$ even. Then we know the eigenvalues are
$$ \lambda_{1, 2} = \frac{-r \pm \sqrt{r^2 - 4}}{2} $$
If $0 < r < 2$ then we will have complex eigenvalues implying that we will have a spiral. Since the real part is positive, solutions are going to spiral in and by looking at the first column we can even infer that spiral is going to be clockwise. This corresponds with the angle tending towards 0 and its speed decreasing, as we would expect the pendulum to behave. If $r > 2$ then we get 2 real eigenvalues, both of which are negative. In this case all equilibria will be stable. This corresponds with the friction becoming so strong that the pendulum can actually become stuck and 'stable' at odd angles. 

We get some rather interesting behaviour at $r = 0$, in the frictionless case. In this case we get a center which again should make sense. If there is no friction, then the pendulum continues swinging on its path ad infinitium. 

\section{Exact Differential Equations}\label{sec:exact-diff-eq}
Suppose we have an ODE of the form
$$ P(x, y)dx + Q(x, y)dy = 0 $$
is exact if there exists a function $f(x, y)$ such that
$$ \frac{\partial f}{\partial x} = P(x, y), \frac{\partial f}{\partial y} = Q(x, y) $$
The general solution to such ODEs is given by the one-parameter family
$$ f(x, y) = c $$

It would be nice to know when an ODE is exact. Luckily we have the following theorem which not only tells us when an equation is exact but even gives a construction for the function $f$.
\begin{theorem}
Suppose we have an ODE of the form
$$ P(x, y)dx + Q(x, y)dy = 0 $$
where $P, Q$ are defined on a simply connected region and are $C^1$. Then the equation is exact if and only if
$$ \frac{\partial P}{\partial y} = \frac{\partial Q}{\partial x} $$
\end{theorem}
\begin{proof}
The forward direction follows from a theorem in analysis. Namely if a function $f$ is smooth (or even just $C^1$) then
\begin{align*}
    \frac{\partial^2 f}{\partial x \partial y} = \frac{\partial^2 f}{\partial y \partial x}
\end{align*}

Thus we need show the reverse direction. Thus we assume that
$$ \frac{\partial P}{\partial y} = \frac{\partial Q}{\partial x} $$
Suppose such an $f$ were to exist. Then we know that $f$ must satisfy
\begin{align*}
    \frac{\partial f}{\partial x} = P(x, y) \Rightarrow f(x, y) = \int_{x_0}^{x} P(x, y)dx + R(y)
\end{align*}
Typically we get a constant term when integrating, but in this case the constant term depends on $y$, hence why $R$ becomes a function of $y$. By assumption the derivative of $f$ with respect to $y$ is $Q$. Thus we get that
\begin{align*}
    Q(x, y) = \frac{\partial f}{\partial y} &= \frac{\partial}{\partial y} \left( \int_{x_0}^{x} P(s, y)ds + R(y) \right)\\
    &= \int_{x_0}^{x} \frac{\partial}{\partial y} P(s, y)ds + R'(y)\\
    &= \int_{x_0}^x \frac{\partial }{\partial x} Q(s, y) ds + R'(y)\\
    &= Q(x, y) - Q(x_0, y) + R'(y)
\end{align*}
where of course $x_0$ is some arbitrary constant in the domain. We then conclude that $R'(y) = Q(x_0, y)$. Therefore
$$ R(y) = \int_{y_0}^{y} Q(x_0, s) ds $$
(we ignore the integration constant for now since account for it later when giving the general solution to the ODE). Therefore
$$ f(x, y) = \int_{x_0}^{x} P(s, y)ds + \int_{y_0}^{y} Q(x_0, s)ds $$
All that remains to check is that this $f$ does indeed satisfy the conditions. Now it is clear that
$$ \frac{\partial f}{\partial x} = P(x, y) $$
The other one takes slightly more work
\begin{align*}
    \frac{\partial f}{\partial y} &= \frac{\partial }{\partial y} \int_{x_0}^x P(s, y) ds + Q(x_0, y)\\
    &= \int_{x_0}^x \frac{\partial }{\partial y} P(s, y) ds + Q(x_0, y)\\
    &= \int_{x_0}^{x} \frac{\partial }{\partial x} Q(s, y) ds + Q(x_0, y)\\
    &= Q(x, y) - Q(x_0, y) + Q(x_0, y)\\
    &= Q(x, y)
\end{align*}
\end{proof}

\subsection{Example}
Suppose we have the equation
\begin{equation}\label{eq:exact-eg}
    (2x + y \cos x)dx + (2y + \sin x - \sin y)dy = 0
\end{equation}
In this case we have $P(x, y) = 2x + y \cos x$ and $Q(x, y) = 2y + \sin x - \sin y$. Since
$$ \frac{\partial P}{\partial y} = \cos x = \frac{\partial Q}{\partial x} $$
we can conclude using the previous theorem that the differential equation is exact. Thus we can construct $f$ as given by the proof as well. We start with
$$ f(x, y) = \int P(x, y) dx + R(y) = x^2 + y \sin x + R(y) $$
Then we know that
\begin{align*}
    \frac{\partial f}{\partial y} &= Q(x, y)\\
    \sin x + R'(y) = 2y + \sin x - \sin y
\end{align*}
Therefore
$$R(y) = y^2 + \cos y$$
Finally we conclude that the general solution to \autoref{eq:exact-eg} is 
$$ f(x, y) = x^2 + y \sin x + y^2 + \cos y = c $$

\subsection{Integrating Factors}
Sometimes an equation is not exact, but we can make it exact by multiplying it with a function. Such a function is called the integration factor. As an example consider the equation
\begin{equation}
    (t^2 x - t) dx + x dt = 0
\end{equation}
Suppose there was some function $h(t)$ (we assume $h$ is a function of $t$ because we know this works. In general it's difficult to know what the integration factor should be a function of). Then we would have
$$ (h(t)(t^2 x - t))dx + (x h(t)) dt = 0 $$
which we assume to be exact. This means that
\begin{align*}
    \frac{\partial }{\partial t} (h(t)(t^2 x - t)) &= \frac{\partial}{\partial x} (x h(t))\\
    h'(t) (t^2 x - t) + h(t) (2tx - 1) &= h(t)\\
    \frac{h'(t)}{h(t)} &= \frac{2 - 2tx}{t^2 x - t} \\
    &= -\frac{2}{t}
\end{align*}
Since the right hand side is a function of $t$, we can integrate to conclude that
$$ \log(h(t)) = -2 \log t $$
or in other words that
$$ h(t) = \frac{1}{t^2} $$

Similar manipulations can be performed if $h$ is function of $x, xt, \frac{x}{t}, \frac{t}{x},$ etc. Often it is not obvious what $h$ should be a function and requires some trial and error. Hence why although we know that any ODE $x' = f(x, t)$ with $f \in C^1$ in a neighbourhood of $(x_0, t_0)$ admits an integration factor, in principle this is a useless technique because it is incredibly hard to find the integration factor.\\

However there do exist some special cases where integration factors \textit{can} be found and these serve as useful examples. Suppose we have an equation of the form
\begin{equation}\label{eq:lin-order-1}
    \frac{dy}{dx} + P(x)y = Q(x)
\end{equation}
Since the derivative and $y$ are raised to the power of $1$, we call this a linear differential equation and specifically a linear differential equation of order 1. One way of solving this equation is to solve the homogeneous version (i.e. set $Q = 0$) using separation of variables and then solve the ODE using variation of constants. A second method, however, is to use integration factors. 

We can first rewrite the equation to get
$$ [P(x) y - Q(x)] dx + dy = 0 $$
Suppose we had an an integrating factor $u(x)$ (in this case we know that the integrating factor is always a function of the dependent variable, in this case $x$). Then we would have
\begin{align*}
    \frac{\partial}{\partial y} [u(x) P(x) y - u(x)Q(x)] &= \frac{\partial}{\partial x} u(x)\\
    u(x) P(x) &= u'(x)\\
    \frac{u'(x)}{u(x)} &= P(x)\\
    u(x) = e^{\int P(x) dx}
\end{align*}
Note that when integrating the exponent we don't need to worry about the integration constant since we don't need the general solution. Substituting this integration factor into \autoref{eq:lin-order-1}  we get
\begin{align*}
    e^{\int P(x) dx} \frac{dy}{dx} + P(x) e^{\int P(x) dx} y &= e^{\int P(x) dx} Q(x)\\
    \frac{d}{dx} (y e^{\int P(x) dx}) &=  e^{\int P(x) dx} Q(x)\\
    y &= e^{- \int P(x) dx} \left( \int e^{\int P(x) dx} Q(x) \right) + c e^{-\int P(x) dx}
\end{align*}
We apologise for the horror we have bestowed upon the reader.